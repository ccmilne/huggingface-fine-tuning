{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2244666b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import AutoTokenizer\n",
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq \n",
    "from transformers import Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "import datasets\n",
    "from datasets import load_from_disk, load_metric\n",
    "import random\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import numpy as np\n",
    "import torch\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3d44b5b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset big_patent (/home/ccmilne/.cache/huggingface/datasets/big_patent/h/1.0.0/bdefa7c0b39fba8bba1c6331b70b738e30d63c8ad4567f983ce315a5fef6131c)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73df0d0ea47b480ab8cefbc6ac7abf03",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset('big_patent', 'h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f1b4e011",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = 'sshleifer/distilbart-xsum-12-1' #'sshleifer/distill-pegasus-xsum-16-4' #\"t5-small\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "628ef92a",
   "metadata": {},
   "source": [
    "## Processing the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31ebba43",
   "metadata": {},
   "outputs": [],
   "source": [
    "if model_checkpoint in [\"t5-small\", \"t5-base\", \"t5-larg\", \"t5-3b\", \"t5-11b\"]:\n",
    "    prefix = \"summarize: \"\n",
    "else:\n",
    "    prefix = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ab2edbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 1024\n",
    "max_target_length = 128\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"description\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(examples[\"abstract\"], max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aaac522f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bb15a9c15bf4499ac1c454e6988ffbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/258 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2135fd2bfe014082beb4d254b8a28570",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ba380ef41234ac599abf5d7ee13b34f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/15 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d7186f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.save_to_disk(\"processed/big_patent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b985fe5",
   "metadata": {},
   "source": [
    "## Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "467dd292",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset = load_from_disk(\"processed/big_patent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0ce22ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 3\n",
    "model_name = model_checkpoint.split(\"/\")[-1]\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    f\"{model_name}-finetuned\",\n",
    "    \n",
    "    #Training Loss\n",
    "    save_strategy = \"steps\",\n",
    "    logging_strategy = 'steps',\n",
    "    logging_steps = 200,\n",
    "\n",
    "    #Validation Loss\n",
    "    evaluation_strategy = 'no', #\"epoch\",\n",
    "#     eval_steps = 10,\n",
    "    \n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=1,\n",
    "    predict_with_generate=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4967e8bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1ba6296",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d009ee41",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    # Replace -100 in the labels as we can't decode them.\n",
    "    labels = np.where(labels != -100, labels, tokenizer.pad_token_id)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    # Rouge expects a newline after each sentence\n",
    "    decoded_preds = [\"\\n\".join(nltk.sent_tokenize(pred.strip())) for pred in decoded_preds]\n",
    "    decoded_labels = [\"\\n\".join(nltk.sent_tokenize(label.strip())) for label in decoded_labels]\n",
    "    \n",
    "    result = metric.compute(predictions=decoded_preds, references=decoded_labels, use_stemmer=True)\n",
    "    # Extract a few results\n",
    "    result = {key: value.mid.fmeasure * 100 for key, value in result.items()}\n",
    "    \n",
    "    # Add mean generated length\n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in predictions]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    \n",
    "    return {k: round(v, 4) for k, v in result.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1284fd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=tokenized_dataset[\"train\"],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8f6eb6fa",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the training set  don't have a corresponding argument in `BartForConditionalGeneration.forward` and have been ignored: abstract, description. If abstract, description are not expected by `BartForConditionalGeneration.forward`,  you can safely ignore this message.\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "***** Running training *****\n",
      "  Num examples = 257019\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 3\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 6\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 42837\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='42837' max='42837' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [42837/42837 7:01:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>4.977400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>4.484600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>4.333200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>4.267300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.228600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>4.117700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>4.073600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>4.045800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>4.001700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>3.935100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>3.882400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>3.903900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>3.856800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>3.803500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>3.815200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3200</td>\n",
       "      <td>3.847100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3400</td>\n",
       "      <td>3.760100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3600</td>\n",
       "      <td>3.739200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3800</td>\n",
       "      <td>3.754600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>3.671500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4200</td>\n",
       "      <td>3.725100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4400</td>\n",
       "      <td>3.674300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4600</td>\n",
       "      <td>3.645200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4800</td>\n",
       "      <td>3.668500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>3.619600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5200</td>\n",
       "      <td>3.643400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5400</td>\n",
       "      <td>3.615100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5600</td>\n",
       "      <td>3.570700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5800</td>\n",
       "      <td>3.576500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>3.613800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6200</td>\n",
       "      <td>3.572900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6400</td>\n",
       "      <td>3.542000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6600</td>\n",
       "      <td>3.494000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6800</td>\n",
       "      <td>3.548500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>3.522800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7200</td>\n",
       "      <td>3.556500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7400</td>\n",
       "      <td>3.512700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7600</td>\n",
       "      <td>3.498200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7800</td>\n",
       "      <td>3.523700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>3.491700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8200</td>\n",
       "      <td>3.494200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8400</td>\n",
       "      <td>3.525700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8600</td>\n",
       "      <td>3.464900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8800</td>\n",
       "      <td>3.487200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>3.465900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9200</td>\n",
       "      <td>3.451000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9400</td>\n",
       "      <td>3.449400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9600</td>\n",
       "      <td>3.410600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9800</td>\n",
       "      <td>3.448000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>3.446600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10200</td>\n",
       "      <td>3.413200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10400</td>\n",
       "      <td>3.426500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10600</td>\n",
       "      <td>3.430900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10800</td>\n",
       "      <td>3.386400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>3.405500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11200</td>\n",
       "      <td>3.415000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11400</td>\n",
       "      <td>3.363600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11600</td>\n",
       "      <td>3.368900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11800</td>\n",
       "      <td>3.379100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>3.396000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12200</td>\n",
       "      <td>3.386100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12400</td>\n",
       "      <td>3.393600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12600</td>\n",
       "      <td>3.363200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12800</td>\n",
       "      <td>3.366800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>3.347200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13200</td>\n",
       "      <td>3.361300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13400</td>\n",
       "      <td>3.346600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13600</td>\n",
       "      <td>3.380000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13800</td>\n",
       "      <td>3.346800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>3.361800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14200</td>\n",
       "      <td>3.388400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14400</td>\n",
       "      <td>3.315700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14600</td>\n",
       "      <td>3.337800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14800</td>\n",
       "      <td>3.327500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15000</td>\n",
       "      <td>3.312300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15200</td>\n",
       "      <td>3.329900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15400</td>\n",
       "      <td>3.363700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15600</td>\n",
       "      <td>3.289300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15800</td>\n",
       "      <td>3.321900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16000</td>\n",
       "      <td>3.298500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16200</td>\n",
       "      <td>3.322800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16400</td>\n",
       "      <td>3.306500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16600</td>\n",
       "      <td>3.311800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16800</td>\n",
       "      <td>3.294200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17000</td>\n",
       "      <td>3.290700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17200</td>\n",
       "      <td>3.287700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17400</td>\n",
       "      <td>3.292600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17600</td>\n",
       "      <td>3.302300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17800</td>\n",
       "      <td>3.279700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18000</td>\n",
       "      <td>3.290600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18200</td>\n",
       "      <td>3.259300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18400</td>\n",
       "      <td>3.259100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18600</td>\n",
       "      <td>3.288300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18800</td>\n",
       "      <td>3.286400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19000</td>\n",
       "      <td>3.275600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19200</td>\n",
       "      <td>3.292700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19400</td>\n",
       "      <td>3.262200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19600</td>\n",
       "      <td>3.203800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19800</td>\n",
       "      <td>3.247200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20000</td>\n",
       "      <td>3.246700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20200</td>\n",
       "      <td>3.224500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20400</td>\n",
       "      <td>3.251800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20600</td>\n",
       "      <td>3.273500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20800</td>\n",
       "      <td>3.235300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21000</td>\n",
       "      <td>3.239800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21200</td>\n",
       "      <td>3.242200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21400</td>\n",
       "      <td>3.236000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21600</td>\n",
       "      <td>3.213900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21800</td>\n",
       "      <td>3.251300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22000</td>\n",
       "      <td>3.199000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22200</td>\n",
       "      <td>3.194500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22400</td>\n",
       "      <td>3.219600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22600</td>\n",
       "      <td>3.209100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22800</td>\n",
       "      <td>3.226400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23000</td>\n",
       "      <td>3.195600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23200</td>\n",
       "      <td>3.195800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23400</td>\n",
       "      <td>3.218700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23600</td>\n",
       "      <td>3.224700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23800</td>\n",
       "      <td>3.218500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24000</td>\n",
       "      <td>3.197200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24200</td>\n",
       "      <td>3.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24400</td>\n",
       "      <td>3.210100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24600</td>\n",
       "      <td>3.164800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24800</td>\n",
       "      <td>3.210100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25000</td>\n",
       "      <td>3.199200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25200</td>\n",
       "      <td>3.224300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25400</td>\n",
       "      <td>3.193400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25600</td>\n",
       "      <td>3.227500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25800</td>\n",
       "      <td>3.144700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26000</td>\n",
       "      <td>3.153400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26200</td>\n",
       "      <td>3.204500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26400</td>\n",
       "      <td>3.173600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26600</td>\n",
       "      <td>3.205300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26800</td>\n",
       "      <td>3.189600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27000</td>\n",
       "      <td>3.172400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27200</td>\n",
       "      <td>3.183200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27400</td>\n",
       "      <td>3.198600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27600</td>\n",
       "      <td>3.164300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27800</td>\n",
       "      <td>3.191500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28000</td>\n",
       "      <td>3.171400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28200</td>\n",
       "      <td>3.200200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28400</td>\n",
       "      <td>3.163400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28600</td>\n",
       "      <td>3.183500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28800</td>\n",
       "      <td>3.169100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29000</td>\n",
       "      <td>3.161700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29200</td>\n",
       "      <td>3.179000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29400</td>\n",
       "      <td>3.148600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29600</td>\n",
       "      <td>3.151600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29800</td>\n",
       "      <td>3.173400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30000</td>\n",
       "      <td>3.170700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30200</td>\n",
       "      <td>3.146500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30400</td>\n",
       "      <td>3.163800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30600</td>\n",
       "      <td>3.145900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30800</td>\n",
       "      <td>3.181100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31000</td>\n",
       "      <td>3.155100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31200</td>\n",
       "      <td>3.156000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31400</td>\n",
       "      <td>3.174700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31600</td>\n",
       "      <td>3.196300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31800</td>\n",
       "      <td>3.159700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32000</td>\n",
       "      <td>3.165000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32200</td>\n",
       "      <td>3.154800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32400</td>\n",
       "      <td>3.161000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32600</td>\n",
       "      <td>3.134300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32800</td>\n",
       "      <td>3.132200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33000</td>\n",
       "      <td>3.157700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33200</td>\n",
       "      <td>3.181300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33400</td>\n",
       "      <td>3.171300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33600</td>\n",
       "      <td>3.136800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33800</td>\n",
       "      <td>3.133600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34000</td>\n",
       "      <td>3.177800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34200</td>\n",
       "      <td>3.153900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34400</td>\n",
       "      <td>3.138300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34600</td>\n",
       "      <td>3.167500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34800</td>\n",
       "      <td>3.131600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35000</td>\n",
       "      <td>3.119700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35200</td>\n",
       "      <td>3.129600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35400</td>\n",
       "      <td>3.103900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35600</td>\n",
       "      <td>3.144100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35800</td>\n",
       "      <td>3.142400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36000</td>\n",
       "      <td>3.119300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36200</td>\n",
       "      <td>3.154900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36400</td>\n",
       "      <td>3.152800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36600</td>\n",
       "      <td>3.121700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36800</td>\n",
       "      <td>3.108600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37000</td>\n",
       "      <td>3.160900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37200</td>\n",
       "      <td>3.146600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37400</td>\n",
       "      <td>3.136700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37600</td>\n",
       "      <td>3.104700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37800</td>\n",
       "      <td>3.136300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38000</td>\n",
       "      <td>3.120300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38200</td>\n",
       "      <td>3.117500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38400</td>\n",
       "      <td>3.104400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38600</td>\n",
       "      <td>3.148600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38800</td>\n",
       "      <td>3.108900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39000</td>\n",
       "      <td>3.132700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39200</td>\n",
       "      <td>3.115600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39400</td>\n",
       "      <td>3.155300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39600</td>\n",
       "      <td>3.098600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39800</td>\n",
       "      <td>3.146100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40000</td>\n",
       "      <td>3.172400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40200</td>\n",
       "      <td>3.115500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40400</td>\n",
       "      <td>3.135400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40600</td>\n",
       "      <td>3.144300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40800</td>\n",
       "      <td>3.105100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41000</td>\n",
       "      <td>3.115900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41200</td>\n",
       "      <td>3.129500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41400</td>\n",
       "      <td>3.109400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41600</td>\n",
       "      <td>3.131800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41800</td>\n",
       "      <td>3.112800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42000</td>\n",
       "      <td>3.120900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42200</td>\n",
       "      <td>3.141400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42400</td>\n",
       "      <td>3.122700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42600</td>\n",
       "      <td>3.067100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42800</td>\n",
       "      <td>3.117900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-41500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-1000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-1000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-1000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-42000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-1500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-1500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-1500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-42500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-2000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-2000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-2500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-2500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-1000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-3000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-3000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-1500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-3500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-3500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-2000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-4000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-4000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-2500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-4500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-4500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-3000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-5000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-5000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-3500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-5500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-5500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-4000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-6000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-6000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-4500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-6500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-6500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-5000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-7000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-7000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-5500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-7500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-7500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-6000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-8000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-8000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-6500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-8500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-8500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-7000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-9000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-9000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-7500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-9500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-9500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-8000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-10000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-10000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-8500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-10500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-10500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-9000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-11000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-11000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-9500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-11500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-11500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-11500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-10000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-12000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-12000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-10500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-12500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-12500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-12500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-11000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-13000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-13000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-13000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-11500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-13500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-13500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-13500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-12000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-14000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-14000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-12500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-14500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-14500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-14500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-14500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-14500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-13000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-15000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-15000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-15000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-15000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-15000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-13500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-15500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-15500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-15500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-15500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-15500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-14000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-16000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-16000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-16000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-16000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-16000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-14500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-16500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-16500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-16500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-16500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-16500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-15000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-17000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-17000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-17000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-17000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-17000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-15500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-17500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-17500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-17500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-17500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-17500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-16000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-18000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-18000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-18000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-18000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-18000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-16500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-18500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-18500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-18500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-18500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-18500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-17000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-19000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-19000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-19000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-19000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-19000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-17500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-19500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-19500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-19500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-19500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-19500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-18000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-20000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-20000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-20000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-20000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-20000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-18500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-20500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-20500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-20500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-20500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-20500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-19000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-21000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-21000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-21000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-21000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-21000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-19500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-21500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-21500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-21500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-21500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-21500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-20000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-22000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-22000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-22000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-22000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-22000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-20500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-22500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-22500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-22500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-22500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-22500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-21000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-23000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-23000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-23000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-23000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-23000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-21500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-23500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-23500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-23500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-23500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-23500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-22000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-24000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-24000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-24000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-24000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-24000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-22500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-24500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-24500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-24500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-24500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-24500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-23000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-25000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-25000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-25000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-25000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-25000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-23500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-25500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-25500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-25500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-25500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-25500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-24000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-26000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-26000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-26000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-26000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-26000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-24500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-26500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-26500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-26500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-26500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-26500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-25000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-27000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-27000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-27000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-27000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-27000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-25500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-27500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-27500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-27500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-27500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-27500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-26000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-28000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-28000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-28000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-28000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-28000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-26500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-28500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-28500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-28500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-28500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-28500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-27000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-29000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-29000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-29000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-29000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-29000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-27500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-29500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-29500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-29500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-29500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-29500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-28000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-30000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-30000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-30000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-30000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-30000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-28500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-30500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-30500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-30500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-30500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-30500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-29000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-31000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-31000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-31000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-31000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-31000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-29500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-31500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-31500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-31500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-31500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-31500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-30000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-32000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-32000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-32000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-32000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-32000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-30500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-32500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-32500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-32500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-32500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-32500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-31000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-33000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-33000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-33000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-33000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-33000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-31500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-33500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-33500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-33500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-33500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-33500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-32000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-34000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-34000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-34000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-34000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-34000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-32500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-34500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-34500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-34500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-34500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-34500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-33000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-35000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-35000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-35000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-35000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-35000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-33500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-35500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-35500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-35500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-35500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-35500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-34000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-36000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-36000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-36000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-36000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-36000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-34500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-36500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-36500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-36500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-36500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-36500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-35000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-37000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-37000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-37000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-37000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-37000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-35500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-37500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-37500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-37500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-37500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-37500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-36000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-38000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-38000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-38000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-38000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-38000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-36500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-38500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-38500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-38500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-38500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-38500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-37000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-39000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-39000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-39000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-39000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-39000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-37500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-39500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-39500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-39500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-39500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-39500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-38000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-40000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-40000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-40000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-40000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-40000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-38500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-40500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-40500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-40500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-40500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-40500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-39000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-41000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-41000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-41000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-41000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-41000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-39500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-41500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-41500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-41500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-41500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-41500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-40000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-42000\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-42000/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-42000/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-42000/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-42000/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-40500] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "Saving model checkpoint to distilbart-xsum-12-1-finetuned/checkpoint-42500\n",
      "Configuration saved in distilbart-xsum-12-1-finetuned/checkpoint-42500/config.json\n",
      "Model weights saved in distilbart-xsum-12-1-finetuned/checkpoint-42500/pytorch_model.bin\n",
      "tokenizer config file saved in distilbart-xsum-12-1-finetuned/checkpoint-42500/tokenizer_config.json\n",
      "Special tokens file saved in distilbart-xsum-12-1-finetuned/checkpoint-42500/special_tokens_map.json\n",
      "Deleting older checkpoint [distilbart-xsum-12-1-finetuned/checkpoint-41000] due to args.save_total_limit\n",
      "/home/ccmilne/.local/lib/python3.8/site-packages/torch/nn/parallel/_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=42837, training_loss=3.3366516485160136, metrics={'train_runtime': 25318.5563, 'train_samples_per_second': 10.151, 'train_steps_per_second': 1.692, 'total_flos': 2.652226151157596e+17, 'train_loss': 3.3366516485160136, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a4a79bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions = trainer.predict(tokenized_dataset[\"test\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d8f411f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PredictionOutput(predictions=array([[    0,     3,     9, ...,     3,  8499,     3],\n",
       "       [    0,     3,     9, ..., 19972,     3,     5],\n",
       "       [    0,     3,     9, ...,  1904,     3,     6],\n",
       "       ...,\n",
       "       [    0,     3,     9, ...,   579,  1899, 15786],\n",
       "       [    0,     3,     9, ...,    44,  5590,     7],\n",
       "       [    0,     3,     9, ...,     9, 16188, 14286]]), label_ids=array([[    3,     9,  3317, ...,    84, 10446,     1],\n",
       "       [    3,     9,  1573, ...,     3,   117,     1],\n",
       "       [    3,     9, 23795, ...,  -100,  -100,  -100],\n",
       "       ...,\n",
       "       [    3,     9,  1573, ...,  7415,     8,     1],\n",
       "       [    3,     9,  3240, ...,   689,    21,     1],\n",
       "       [    3,     9,     3, ...,  -100,  -100,  -100]]), metrics={'test_loss': 2.4342410564422607, 'test_rouge1': 17.7916, 'test_rouge2': 6.4898, 'test_rougeL': 14.9019, 'test_rougeLsum': 16.0102, 'test_gen_len': 19.0, 'test_runtime': 640.4784, 'test_samples_per_second': 22.294, 'test_steps_per_second': 2.787})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95b74019",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preds = np.argmax(predictions.predictions, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b20d4207",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to trained_models/bart_trained\n",
      "Configuration saved in trained_models/bart_trained/config.json\n",
      "Model weights saved in trained_models/bart_trained/pytorch_model.bin\n",
      "tokenizer config file saved in trained_models/bart_trained/tokenizer_config.json\n",
      "Special tokens file saved in trained_models/bart_trained/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "trainer.save_model('trained_models/bart_trained')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
